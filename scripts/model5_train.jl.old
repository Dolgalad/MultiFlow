ENV["JULIA_CUDA_MEMORY_POOL"] = "none"
ENV["GKSwstype"] = "nul"

using Plots
using GraphNeuralNetworks, Graphs, Flux, CUDA, Statistics, MLUtils
using Flux: DataLoader
using Flux.Losses: logitbinarycrossentropy, binary_focal_loss, mean, logsoftmax

using UMFSolver
using Distributions
using TensorBoardLogger
using Logging
using LinearAlgebra

using BSON: @save

device = CUDA.functional() ? Flux.gpu : Flux.cpu;

println("CUDA.functional: ", CUDA.functional())

workdir = "."
mkpath(workdir)


accuracy(y_pred, y_true)     = sum(y_pred .== y_true)/size(y_pred, 1)
tpcount(y_pred, y_true)      = sum( Bool.(y_pred .== y_true) .& Bool.(y_true)  )
precision(y_pred, y_true)    = sum(y_pred)>0 ? tpcount(y_pred, y_true)/sum(y_pred) : 0
recall(y_pred, y_true)       = sum(y_true)>0 ? tpcount(y_pred, y_true)/sum(y_true) : 0
f_beta_score(y_pred, y_true, beta=1.) = precision(y_pred,y_true)>0. && recall(y_pred,y_true)>0. ? sum(1 .+ (beta^2)) * (precision(y_pred,y_true)*recall(y_pred,y_true))/(((beta^2) * precision(y_pred,y_true)) + recall(y_pred,y_true)) : 0
graph_reduction(y_pred)      = 1. - sum(y_pred)/prod(size(y_pred))


function train_model(epochs, nhidden, 
		      nlayers, 
		      device, 
		      lr, 
		      dataset_path, 
		      model_name, 
		      tb_log_dir,
		      eval_dataset,
		      es_patience,
		      weight
		      )
    dataset_name = basename(dataset_path)
    eval_dataset_name = basename(eval_dataset)

    println("\tnhidden            = ", nhidden)
    println("\tnlayers            = ", nlayers)
    println("\tdevice             = ", device)
    println("\tlr                 = ", lr)
    println("\tdataset            = ", dataset_path)
    println("\tdataset name       = ", dataset_name)
    println("\tmodel name         = ", model_name)
    println("\tlog dir            = ", tb_log_dir)
    println("\tevaluation dataset = ", eval_dataset)
    println("\tevaluation dataset name = ", eval_dataset_name)
    println("\tES patience        = ", es_patience)
    println("\tweight             = ", weight)
    save_path = joinpath(workdir, "models", dataset_name, model_name)
    mkpath(save_path)
    println("\tmodel save path    = ", save_path)


    best_f1 = -1.0f8
    
    all_graphs = GNNGraph[]
    all_graphs = UMFSolver.load_dataset(dataset_path)


    # count number of nodes in the graph
    nnodes = sum(all_graphs[1].ndata.mask)
    println("\tnnodes             = ", nnodes)

    model = UMFSolver.M5ClassifierModel(nhidden, 3, nlayers, nnodes) |> device
    
    ps = Flux.params(model)
    
    opt = Flux.Optimise.Optimiser(ClipNorm(1.), Adam(lr))
    opt = Adam(lr)

    
    train_graphs, test_graphs = MLUtils.splitobs(all_graphs, at=0.9)
    
    train_loader = DataLoader(train_graphs,
                    batchsize=1, shuffle=true, collate=true)
    test_loader = DataLoader(test_graphs,
                   batchsize=1, shuffle=false, collate=true)
    
    
    function metrics(loader)
        preds = [(Int64.(cpu((vec(model(g |> device)) .> 0))),Int64.(vec(g.targets))) for g in loader]
        acc = mean([accuracy(pred, lab) for (pred,lab) in preds])
        rec = mean([recall(pred, lab) for (pred,lab) in preds])
        prec = mean([precision(pred, lab) for (pred,lab) in preds])
        f1 = mean([f_beta_score(pred, lab) for (pred,lab) in preds])
        gr = mean([graph_reduction(pred) for (pred,lab) in preds])
    
        return (acc=acc, rec=rec, prec=prec, f1=f1, gr=gr)
    end
    
    
    logger = TBLogger(tb_log_dir, tb_overwrite)
    
    function node_distance_matrix(codes; dist_func=(x,y) -> norm(x-y))
        nnodes = size(codes, 2)
        m = zeros(nnodes, nnodes)
        for i in 1:nnodes
            for j in 1:nnodes
                if i!=j
                    m[i,j] = dist_func(codes[:,i], codes[:,j])
                end
            end
        end
        return m
    end 

    function TBCallback(epoch)
        train_loss = loss(train_loader)
        test_loss = loss(test_loader)
        train_metrics = metrics(train_loader)
        test_metrics = metrics(test_loader)

        # image on test graph 1
	g = test_graphs[1]
        node_codes,_ = UMFSolver.m5_compute_graph_embeddings(model, g |> device)
        nedm = node_distance_matrix(Flux.cpu(node_codes))
	dne = Flux.cpu(UMFSolver.make_demand_codes(node_codes, g))
        #adj_mat = (adjacency_matrix(g, dir=:in) .+ adjacency_matrix(g, dir=:out)) .> 0
        #plt=plot(heatmap(adj_mat, legend=:none,title="Augmented graph adjacency matrix"), heatmap(nedm, title="Node embedding distances"), size=(500,500));
        plt=plot(heatmap(nedm, title="Node embedding distances"), size=(500,500));

	ds,dt = UMFSolver.demand_endpoints(g)
	demand_ep = hcat(ds, dt)
        demand_sorted_idx = sortperm(view.(Ref(demand_ep), 1:size(demand_ep, 1), :))

	nnodes = sum(g.ndata.mask)
	ndemands = nv(g) - nnodes
        m = zeros(Bool, ndemands, nnodes)
        for k in 1:ndemands
            m[k, ds[demand_sorted_idx[k]]] = 1
            m[k, dt[demand_sorted_idx[k]]] = 1
	end
        #ncats = 2*length(unique(dedm))
	dedm = node_distance_matrix(dne[:,demand_sorted_idx])
	dd_plt=plot(heatmap(m, title="Demand-node adjacency", legend=:none),heatmap(dedm,title="Demand embedding distances", cmap=cgrad(:lighttest,categorical=false), showaxis=:x), size=(1000,500));


        #println("train_loss: ", typeof(train_loss))
        #println("test loss : ", typeof(test_loss))
        #println("train_metrics: ", typeof(train_metrics))
        #println("test metrics : ", typeof(test_metrics))

        with_logger(logger) do
            @info "train" loss=train_loss train_metrics... log_step_increment=0
            @info "test" loss=test_loss test_metrics...
            @info "test_img" adj=plt dn_dist=dd_plt
	    @info "memory" cpu=UMFSolver.get_memory_usage() gpu=UMFSolver.get_gpu_memory_usage()
        end
    
    end
    
    
    #loss(g::GNNGraph) = logitbinarycrossentropy(vec(model(g)), vec(g.y))
    loss(g::GNNGraph) = Flux.Losses.tversky_loss(sigmoid(vec(model(g))), vec(g.targets); beta=0.9)
    loss(loader) = mean(loss(g |> device) for g in loader)
    
    # Early stopping
    function es_metric()
        return loss(test_loader)
        return -metrics(test_loader).f1
    end
    es = Flux.early_stopping(es_metric, es_patience, min_dist=1.0f-8, init_score=1.0f8);
    
    
    println("starting training")
    for epoch in 1:epochs
        for g in train_loader
            g = g |> device
            grad = gradient(() -> loss(g), ps)
            #println("GRAD : ", grad)
            if any([sum(isnan.(grad[p] |> cpu)) > 0 for p in Flux.params(model)])
                println([isnan.(grad[p] |> cpu) for p in Flux.params(model)])
                throw("NaN found")
            end
            Flux.Optimise.update!(opt, ps, grad)
        end

        test_metrics = metrics(test_loader)
        @info (; epoch, train_loss=loss(train_loader), test_loss=loss(test_loader), test_metrics=test_metrics)
        TBCallback(epoch)
        # checkpoints
        _model = model |> Flux.cpu
    	@save joinpath(save_path, "checkpoint_e$(epoch).bson") _model
        # early stopping
        es() && break

	GC.gc() ; 
        if CUDA.functional()
            CUDA.reclaim();
        end
    end
    
    # testing: load the testing dataset
    function solve_dataset(dataset_dir::String, output_dir::String, pr::String)
        for f in readdir(dataset_dir, join=true)
            link_f = joinpath(f, "link.csv")
            if !isfile(link_f)
                continue
            end
            sol, stats = solveUMF(f*"/", "CG", "highs", "./output.txt", "", pr)
            stats_path = joinpath(output_dir, basename(f))
            
            UMFSolver.save(stats, stats_path)
            #inst = UMFData(f)
        end
    end
end 



epochs_ = [1000]
nhiddens = [8]
nlayers_ = [4]
devices = [device]
lrs = [1.0e-6]
#train_datasets = ["/data1/schulz/datasets/dataset_prc_small_flexE_1_train"]
#eval_datasets = ["/data1/schulz/datasets/dataset_prc_small_flexE_1_test"]
# DEBUG
train_datasets = ["datasets/dataset_prc_small_flexE_1_train"]
eval_datasets = ["datasets/dataset_prc_small_flexE_1_test"]

weights = [.95]
es_patiences = [100]

# call train_model on a dummy case first to force compilation
dummy_log_dir = "dummy_model_log"
dummy_n = 1
while isdir("$(dummy_log_dir)$(dummy_n)")
    global dummy_n = dummy_n + 1
end
dummy_log_dir = "$(dummy_log_dir)$(dummy_n)"

train_model(1, 2, 1, device, 0.0001, eval_datasets[1], "dummy_model$(dummy_n)", dummy_log_dir, eval_datasets[1], 1, .5)


dataset_names = ["dataset_prc_small_flexE_1",
	         #"dataset_prc_small_flexE_2",
                 #"dataset_prc_small_flexE_3",
                 #"dataset_prc_small_flexE_4",
                 #"dataset_prc_small_flexE_5",
                 #"dataset_prc_small_mixed_1",
	         #"dataset_prc_small_mixed_2",
                 #"dataset_prc_small_mixed_3",
                 #"dataset_prc_small_mixed_4",
                 #"dataset_prc_small_mixed_5",
                 #"dataset_prc_small_vlan_1",
	         #"dataset_prc_small_vlan_2",
                 #"dataset_prc_small_vlan_3",
                 #"dataset_prc_small_vlan_4",
                 #"dataset_prc_small_vlan_5",
                 #"dataset_prc_middle_flexE_1",
	         #"dataset_prc_middle_flexE_2",
                 #"dataset_prc_middle_flexE_3",
                 #"dataset_prc_middle_flexE_4",
                 #"dataset_prc_middle_flexE_5",
	    ]

for name in dataset_names
    train_dataset_path = joinpath(workdir, "datasets", name*"_train")
    test_dataset_path = joinpath(workdir, "datasets", name*"_test")
    # DEBUG
    #train_dataset_path = joinpath(dirname(workdir), "datasets", name*"_train")
    #test_dataset_path = joinpath(dirname(workdir), "datasets", name*"_test")

    if !isdir(train_dataset_path) || !isdir(test_dataset_path)
        println("skipping ", name)
        continue
    end
    model_name = "model5_l"*string(nlayers_[1])*"_lr"*string(lrs[1])*"_h"*string(nhiddens[1])*"_tversky.9"
    # DEBUG
    #model_name = "model5_l"*string(nlayers_[1])*"_lr"*string(lrs[1])*"_h"*string(nhiddens[1])*"_tversky.7_emb0"

    tb_log_dir = joinpath(workdir, "tb_logs", "model5_$(name)_logs", model_name)

    train_model(epochs_[1], nhiddens[1], nlayers_[1], device, lrs[1], train_dataset_path, model_name, tb_log_dir, test_dataset_path, es_patiences[1], .5)

    # reclaim memory
    println("Reclaiming memory")
    @sync GC.gc()
    if CUDA.functional()
        CUDA.reclaim()
    end
    println("done")

end

